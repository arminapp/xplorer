#!/usr/bin/env python3
"""
XGBoost Machine Learning Model for Exoplanet Habitability Prediction
Uses gradient boosting with handling for imbalanced data and missing values.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                             precision_recall_curve, roc_curve, f1_score, accuracy_score)
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Check if xgboost is available, install if needed
try:
    import xgboost as xgb
    print("XGBoost version:", xgb.__version__)
except ImportError:
    print("Installing XGBoost...")
    import subprocess
    subprocess.check_call(['pip', 'install', 'xgboost', '-q'])
    import xgboost as xgb

# Try to import SHAP for interpretability
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    print("SHAP not available. Install with: pip install shap")
    SHAP_AVAILABLE = False


class XGBoostHabitabilityPredictor:
    """
    XGBoost-based machine learning model for predicting exoplanet habitability.
    """

    def __init__(self, data_file: str):
        """Initialize and load data."""
        print("="*80)
        print("XGBOOST HABITABILITY PREDICTION MODEL")
        print("="*80)
        print(f"\nLoading data from: {data_file}")

        # Load the habitability scores generated by physics-based analysis
        self.df = pd.read_csv(data_file)
        print(f"Loaded {len(self.df)} planets")

        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = None

    def prepare_features(self):
        """
        Engineer features for machine learning.
        """
        print("\nPreparing features...")

        # Select and engineer features
        features = pd.DataFrame()

        # Basic physical properties
        features['radius'] = self.df['radius_earth']
        features['mass'] = self.df['estimated_mass_earth']
        features['period'] = self.df['period_days']
        features['temperature'] = self.df['temperature_k']
        features['semi_major_axis'] = self.df['semi_major_axis_au']
        features['insolation'] = self.df['insolation_flux']

        # Derived features
        features['density'] = features['mass'] / (features['radius'] ** 3)  # Proxy for bulk density
        features['escape_velocity'] = np.sqrt(features['mass'] / features['radius'])
        features['surface_gravity'] = features['mass'] / (features['radius'] ** 2)

        # Temperature features
        features['temp_diff_from_earth'] = np.abs(features['temperature'] - 288)
        features['temp_in_liquid_water_range'] = (
            (features['temperature'] >= 273) & (features['temperature'] <= 373)
        ).astype(int)

        # Size categories
        features['earth_like_size'] = (
            (features['radius'] >= 0.8) & (features['radius'] <= 1.4)
        ).astype(int)
        features['super_earth'] = (
            (features['radius'] > 1.4) & (features['radius'] <= 2.0)
        ).astype(int)
        features['mini_neptune'] = (
            (features['radius'] > 2.0) & (features['radius'] <= 4.0)
        ).astype(int)

        # Orbital features
        features['log_period'] = np.log10(features['period'] + 1)
        features['orbital_velocity'] = 2 * np.pi * features['semi_major_axis'] / (features['period'] / 365.25)

        # Insolation categories
        features['goldilocks_insolation'] = (
            (features['insolation'] >= 0.25) & (features['insolation'] <= 1.5)
        ).astype(int)

        # Habitability zone indicators (from physics analysis)
        features['in_hz_conservative'] = self.df['in_hz_conservative'].astype(int)
        features['in_hz_optimistic'] = self.df['in_hz_optimistic'].astype(int)

        # ESI and other scores
        features['esi'] = self.df['esi']
        features['temp_score'] = self.df['temp_score']
        features['size_mass_score'] = self.df['size_mass_score']

        # Handle missing values
        features = features.fillna(features.median())

        # Replace infinities
        features = features.replace([np.inf, -np.inf], np.nan)
        features = features.fillna(features.median())

        self.feature_names = features.columns.tolist()
        print(f"Created {len(self.feature_names)} features")

        return features

    def create_labels(self, threshold: float = 0.4):
        """
        Create binary labels for habitability.
        Planets with habitability_score >= threshold are labeled as potentially habitable.
        """
        labels = (self.df['habitability_score'] >= threshold).astype(int)

        # Only include confirmed and candidate planets (exclude false positives)
        valid_mask = self.df['disposition'].isin(['CONFIRMED', 'CANDIDATE'])

        print(f"\nLabel distribution (threshold={threshold}):")
        print(f"  Potentially habitable: {labels[valid_mask].sum()}")
        print(f"  Not habitable: {(~labels[valid_mask]).sum()}")
        print(f"  Class imbalance ratio: {(~labels[valid_mask]).sum() / labels[valid_mask].sum():.2f}:1")

        return labels, valid_mask

    def train_model(self, X_train, y_train, X_val, y_val):
        """
        Train XGBoost model with optimal hyperparameters.
        """
        print("\nTraining XGBoost model...")

        # Calculate scale_pos_weight for imbalanced data
        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
        print(f"Using scale_pos_weight: {scale_pos_weight:.2f}")

        # XGBoost parameters optimized for habitability prediction
        params = {
            'objective': 'binary:logistic',
            'eval_metric': ['logloss', 'auc'],
            'max_depth': 6,
            'learning_rate': 0.05,
            'n_estimators': 300,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 3,
            'gamma': 0.1,
            'scale_pos_weight': scale_pos_weight,
            'reg_alpha': 0.1,  # L1 regularization
            'reg_lambda': 1.0,  # L2 regularization
            'random_state': 42,
            'tree_method': 'hist',  # Faster training
            'enable_categorical': False,
        }

        # Create DMatrix for XGBoost
        dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=self.feature_names)
        dval = xgb.DMatrix(X_val, label=y_val, feature_names=self.feature_names)

        # Train with early stopping
        evals = [(dtrain, 'train'), (dval, 'validation')]
        self.model = xgb.train(
            params,
            dtrain,
            num_boost_round=1000,
            evals=evals,
            early_stopping_rounds=50,
            verbose_eval=50
        )

        print(f"\nBest iteration: {self.model.best_iteration}")
        print(f"Best score: {self.model.best_score:.4f}")

        return self.model

    def evaluate_model(self, X_test, y_test):
        """
        Evaluate model performance with multiple metrics.
        """
        print("\n" + "="*80)
        print("MODEL EVALUATION")
        print("="*80)

        # Predictions
        dtest = xgb.DMatrix(X_test, feature_names=self.feature_names)
        y_pred_proba = self.model.predict(dtest)
        y_pred = (y_pred_proba >= 0.5).astype(int)

        # Classification metrics
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred, target_names=['Not Habitable', 'Habitable']))

        # Additional metrics
        print(f"\nAccuracy: {accuracy_score(y_test, y_pred):.4f}")
        print(f"F1-Score: {f1_score(y_test, y_pred):.4f}")
        print(f"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        print("\nConfusion Matrix:")
        print(f"                 Predicted Not Hab  Predicted Hab")
        print(f"Actual Not Hab:  {cm[0,0]:8d}          {cm[0,1]:8d}")
        print(f"Actual Hab:      {cm[1,0]:8d}          {cm[1,1]:8d}")

        return y_pred_proba, y_pred

    def cross_validate(self, X, y):
        """
        Perform stratified k-fold cross-validation.
        """
        print("\nPerforming 5-fold cross-validation...")

        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = []

        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
            X_fold_train, X_fold_val = X[train_idx], X[val_idx]
            y_fold_train, y_fold_val = y[train_idx], y[val_idx]

            # Train mini model
            scale_pos_weight = (y_fold_train == 0).sum() / (y_fold_train == 1).sum()

            clf = xgb.XGBClassifier(
                max_depth=6,
                learning_rate=0.05,
                n_estimators=200,
                scale_pos_weight=scale_pos_weight,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                eval_metric='auc'
            )

            clf.fit(X_fold_train, y_fold_train, verbose=False)
            y_fold_pred_proba = clf.predict_proba(X_fold_val)[:, 1]
            auc = roc_auc_score(y_fold_val, y_fold_pred_proba)
            cv_scores.append(auc)

            print(f"  Fold {fold}: AUC = {auc:.4f}")

        print(f"\nCross-validation AUC: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})")

    def plot_feature_importance(self, output_file='feature_importance.png'):
        """
        Plot feature importance from XGBoost model.
        """
        print("\nGenerating feature importance plot...")

        importance = self.model.get_score(importance_type='gain')
        importance_df = pd.DataFrame({
            'feature': list(importance.keys()),
            'importance': list(importance.values())
        }).sort_values('importance', ascending=False).head(20)

        plt.figure(figsize=(10, 8))
        plt.barh(range(len(importance_df)), importance_df['importance'])
        plt.yticks(range(len(importance_df)), importance_df['feature'])
        plt.xlabel('Importance (Gain)')
        plt.title('Top 20 Most Important Features for Habitability Prediction')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"Saved to: {output_file}")
        plt.close()

    def plot_roc_curve(self, y_test, y_pred_proba, output_file='roc_curve.png'):
        """
        Plot ROC curve.
        """
        print("\nGenerating ROC curve...")

        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        auc = roc_auc_score(y_test, y_pred_proba)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'XGBoost (AUC = {auc:.3f})', linewidth=2)
        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve - Habitability Classification')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"Saved to: {output_file}")
        plt.close()

    def plot_precision_recall_curve(self, y_test, y_pred_proba, output_file='pr_curve.png'):
        """
        Plot Precision-Recall curve (better for imbalanced datasets).
        """
        print("\nGenerating Precision-Recall curve...")

        precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)

        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, linewidth=2)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve - Habitability Classification')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"Saved to: {output_file}")
        plt.close()

    def generate_predictions(self, X_full, output_file='xgboost_predictions.csv'):
        """
        Generate predictions for all planets and save results.
        """
        print("\nGenerating predictions for all planets...")

        dfull = xgb.DMatrix(X_full, feature_names=self.feature_names)
        predictions = self.model.predict(dfull)

        # Create results dataframe
        results = self.df.copy()
        results['ml_habitability_probability'] = predictions
        results['ml_habitability_class'] = (predictions >= 0.5).astype(int)

        # Sort by ML probability
        results = results.sort_values('ml_habitability_probability', ascending=False)

        # Save
        results.to_csv(output_file, index=False)
        print(f"Predictions saved to: {output_file}")

        # Show top predictions
        print("\n" + "="*80)
        print("TOP 10 PLANETS BY ML PREDICTION")
        print("="*80)

        top_10 = results.head(10)
        for idx, (i, row) in enumerate(top_10.iterrows(), 1):
            print(f"\n{idx}. {row['planet_name']}")
            print(f"   ML Probability: {row['ml_habitability_probability']:.3f}")
            print(f"   Physics Score:  {row['habitability_score']:.3f}")
            print(f"   Status: {row['disposition']}")
            print(f"   Radius: {row['radius_earth']:.2f} R_Earth, Temp: {row['temperature_k']:.0f} K")

        return results

    def shap_analysis(self, X_test, output_file='shap_summary.png'):
        """
        Perform SHAP analysis for model interpretability.
        """
        if not SHAP_AVAILABLE:
            print("\nSHAP analysis skipped (library not installed)")
            return

        print("\nPerforming SHAP analysis...")

        # Create explainer
        explainer = shap.TreeExplainer(self.model)
        dtest = xgb.DMatrix(X_test, feature_names=self.feature_names)

        # Calculate SHAP values
        shap_values = explainer.shap_values(X_test)

        # Summary plot
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, X_test, feature_names=self.feature_names, show=False)
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"SHAP summary plot saved to: {output_file}")
        plt.close()


def main():
    """Main execution workflow."""

    # Load habitability scores from physics-based analysis
    scores_file = '/Users/armin/Documents/xplorer/cumulative_2025.10.04_15.04.09_habitability_scores.csv'

    # Initialize predictor
    predictor = XGBoostHabitabilityPredictor(scores_file)

    # Prepare features
    X = predictor.prepare_features()

    # Create labels (threshold = 0.4 for moderate-to-high habitability)
    y, valid_mask = predictor.create_labels(threshold=0.4)

    # Filter to valid planets only
    X_filtered = X[valid_mask].values
    y_filtered = y[valid_mask].values

    print(f"\nFiltered dataset: {len(X_filtered)} planets")
    print(f"Features shape: {X_filtered.shape}")

    # Split data
    X_train, X_temp, y_train, y_temp = train_test_split(
        X_filtered, y_filtered, test_size=0.3, random_state=42, stratify=y_filtered
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )

    print(f"\nData split:")
    print(f"  Training:   {len(X_train)} samples")
    print(f"  Validation: {len(X_val)} samples")
    print(f"  Test:       {len(X_test)} samples")

    # Cross-validation
    predictor.cross_validate(X_filtered, y_filtered)

    # Train final model
    model = predictor.train_model(X_train, y_train, X_val, y_val)

    # Evaluate
    y_pred_proba, y_pred = predictor.evaluate_model(X_test, y_test)

    # Visualizations
    predictor.plot_feature_importance('feature_importance.png')
    predictor.plot_roc_curve(y_test, y_pred_proba, 'roc_curve.png')
    predictor.plot_precision_recall_curve(y_test, y_pred_proba, 'pr_curve.png')

    # SHAP analysis
    predictor.shap_analysis(X_test, 'shap_summary.png')

    # Generate predictions for all planets
    X_full = predictor.prepare_features().values
    results = predictor.generate_predictions(X_full, 'xgboost_habitability_predictions.csv')

    # Compare ML vs Physics-based scoring
    print("\n" + "="*80)
    print("ML VS PHYSICS-BASED SCORING COMPARISON")
    print("="*80)

    top_ml = set(results.nlargest(20, 'ml_habitability_probability')['planet_name'])
    top_physics = set(results.nlargest(20, 'habitability_score')['planet_name'])
    overlap = top_ml & top_physics

    print(f"\nTop 20 overlap: {len(overlap)} planets")
    print(f"ML-only discoveries: {len(top_ml - top_physics)}")
    print(f"Physics-only: {len(top_physics - top_ml)}")

    if overlap:
        print("\nPlanets in both top-20 lists:")
        valid_names = [name for name in overlap if pd.notna(name) and isinstance(name, str)]
        for name in sorted(valid_names):
            print(f"  - {name}")

    print("\n" + "="*80)
    print("ANALYSIS COMPLETE")
    print("="*80)


if __name__ == '__main__':
    main()
